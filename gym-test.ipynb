{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class TerraBot(gym.Env):\n",
    "\tdef __init__(self, size=5):\n",
    "\t\tself.size = size  # The size of the square grid\n",
    "\t\tself.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "\t\tself.observation_space = spaces.Box(0, size - 1, shape=(4,), dtype=int)\n",
    "\t\tself.action_space = spaces.Discrete(4)\n",
    "\n",
    "\t\tself._action_to_direction = {\n",
    "\t\t\t0: np.array([-1, 0]),\t# up\n",
    "\t\t\t1: np.array([0, 1]), \t# right\n",
    "\t\t\t2: np.array([1, 0]),\t# down\n",
    "\t\t\t3: np.array([0, -1]),\t# left\n",
    "\t\t}\n",
    "\n",
    "\tdef _get_obs(self):\n",
    "\t\treturn np.concatenate((self._agent_location, self._target_location))\n",
    "\n",
    "\tdef _get_info(self):\n",
    "\t\treturn np.linalg.norm(self._agent_location - self._target_location, ord=1)\n",
    "\n",
    "\tdef reset(self, seed=None, options=None):\n",
    "\t\t# We need the following line to seed self.np_random\n",
    "\t\tsuper().reset(seed=seed)\n",
    "\n",
    "\t\t# Choose the agent's location uniformly at random\n",
    "\t\tself._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "\t\t# We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "\t\tself._target_location = self._agent_location\n",
    "\t\twhile np.array_equal(self._target_location, self._agent_location):\n",
    "\t\t\tself._target_location = self.np_random.integers(\n",
    "\t\t\t\t0, self.size, size=2, dtype=int\n",
    "\t\t\t)\n",
    "\n",
    "\t\tobservation = self._get_obs()\n",
    "\t\tinfo = self._get_info()\n",
    "\n",
    "\t\tif self.render_mode == \"human\":\n",
    "\t\t\tself._render_frame()\n",
    "\n",
    "\t\treturn observation, info\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\t# Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "\t\tdirection = self._action_to_direction[action]\n",
    "\t\t# We use `np.clip` to make sure we don't leave the grid\n",
    "\t\tself._agent_location = np.clip(\n",
    "\t\t\tself._agent_location + direction, 0, self.size - 1\n",
    "\t\t)\n",
    "\t\t# An episode is done iff the agent has reached the target\n",
    "\t\tterminated = np.array_equal(self._agent_location, self._target_location)\n",
    "\t\treward = 1 if terminated else 0  # Binary sparse rewards\n",
    "\t\tobservation = self._get_obs()\n",
    "\t\tinfo = self._get_info()\n",
    "\n",
    "\t\tif self.render_mode == \"human\":\n",
    "\t\t\tself._render_frame()\n",
    "\n",
    "\t\treturn observation, reward, terminated, False, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Define the policy network\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "policy = Policy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(policy.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(rewards, log_probs, optimizer):\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    loss = -torch.mean(log_probs * sum(rewards))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: 1\n",
      "Episode 1000: 1\n",
      "Episode 2000: 1\n",
      "Episode 3000: 1\n",
      "Episode 4000: 1\n",
      "Episode 5000: 1\n",
      "Episode 6000: 1\n",
      "Episode 7000: 1\n",
      "Episode 8000: 1\n",
      "Episode 9000: 1\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10000):\n",
    "\tstate, _ = env.reset()\n",
    "\tdone = False\n",
    "\trewards = []\n",
    "\tlog_probs = []\n",
    "\t\n",
    "\twhile not done:\n",
    "\t\t# Select action\n",
    "\t\tstate = torch.tensor(state, dtype=torch.float32).reshape(1, -1)\n",
    "\t\tprobs = policy(state)\n",
    "\t\taction = torch.multinomial(probs, 1).item()\n",
    "\t\tlog_prob = torch.log(probs[0, action])\n",
    "\n",
    "\t\t# Take step\n",
    "\t\tnext_state, reward, done, _, _ = env.step(action)\n",
    "\t\trewards.append(reward)\n",
    "\t\tlog_probs.append(log_prob)\n",
    "\t\tstate = next_state\n",
    "\t\t\n",
    "\t# Update policy\n",
    "\tif episode % 1000 == 0:\n",
    "\t\tprint(f\"Episode {episode}: {sum(rewards)}\")\n",
    "\tupdate_policy(rewards, log_probs, optimizer)\n",
    "\trewards = []\n",
    "\tlog_probs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 4 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "print(state)\n",
    "\n",
    "state = torch.tensor(state, dtype=torch.float32).reshape(1, -1)\n",
    "probs = policy(state)\n",
    "\n",
    "action = torch.multinomial(probs, 1).item()\n",
    "\n",
    "action\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
