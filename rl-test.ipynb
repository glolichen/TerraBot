{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class TerraBot(gym.Env):\n",
    "\t# [weight, humidity, temperature, light level]\n",
    "\t# TODO find actual optimal weight\n",
    "\tdef __init__(self, size=5):\n",
    "\t\tself.targets = np.array([400, 75, 26, 900])\n",
    "\n",
    "\t\t# SENSORS------------------------------------  ACTUATORS----------------------------\n",
    "\t\t# [weight, humidity, temperature, light level, led strength, fan on/off, pump on/off]\n",
    "\t\tself.observation_space = spaces.Box(0, 1000, shape=(7,), dtype=int)\n",
    "\n",
    "\t\t# (255 - 0 + 1) + 2 + 2 = 256 + 4 + 260\n",
    "\t\tself.action_space = spaces.Discrete(260)\n",
    "\n",
    "\t\tfor i in range(256):\n",
    "\t\t\tself._action_to_actuator[i] = np.array([i, 0, 0])\n",
    "\t\tself._action_to_actuator[256] = [0, 0, 0]\n",
    "\t\tself._action_to_actuator[257] = [0, 1, 0]\n",
    "\t\tself._action_to_actuator[258] = [0, 0, 0]\n",
    "\t\tself._action_to_actuator[259] = [0, 0, 1]\n",
    "\n",
    "\tdef _get_observations(self):\n",
    "\t\treturn np.concatenate((self._sensors, self._actuators))\n",
    "\tdef _get_info(self):\n",
    "\t\treturn np.linalg.norm(self._sensors - self.targets, ord=2)\n",
    "\n",
    "\tdef reset(self, seed=None, options=None):\n",
    "\t\tsuper().reset(seed=seed)\n",
    "\n",
    "\t\tself._sensors = np.array([\n",
    "\t\t\trandom.SystemRandom().randint(0, 1000),\n",
    "\t\t\trandom.SystemRandom().randint(0, 100),\n",
    "\t\t\trandom.SystemRandom().randint(10, 40),\n",
    "\t\t\trandom.SystemRandom().randint(0, 1000)\n",
    "\t\t])\n",
    "\t\tself._actuators = np.array([\n",
    "\t\t\trandom.SystemRandom().randint(0, 255),\n",
    "\t\t\trandom.SystemRandom().randint(0, 1),\n",
    "\t\t\trandom.SystemRandom().randint(0, 1)\n",
    "\t\t])\n",
    "\n",
    "\t\treturn self._get_observations(), self._get_info()\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\tself._actuators = self._action_to_actuator[action]\n",
    "\n",
    "\t\tterminated = np.array_equal(self._agent_location, self._target_location)\n",
    "\t\treward = 1 if terminated else 0  # Binary sparse rewards\n",
    "\t\tobservation = self._get_observations()\n",
    "\t\tinfo = self._get_info()\n",
    "\n",
    "\t\tif self.render_mode == \"human\":\n",
    "\t\t\tself._render_frame()\n",
    "\n",
    "\t\treturn observation, reward, terminated, False, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Define the policy network\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "policy = Policy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(policy.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(rewards, log_probs, optimizer):\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    loss = -torch.mean(log_probs * sum(rewards))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: 1\n",
      "Episode 1000: 1\n",
      "Episode 2000: 1\n",
      "Episode 3000: 1\n",
      "Episode 4000: 1\n",
      "Episode 5000: 1\n",
      "Episode 6000: 1\n",
      "Episode 7000: 1\n",
      "Episode 8000: 1\n",
      "Episode 9000: 1\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10000):\n",
    "\tstate, _ = env.reset()\n",
    "\tdone = False\n",
    "\trewards = []\n",
    "\tlog_probs = []\n",
    "\t\n",
    "\twhile not done:\n",
    "\t\t# Select action\n",
    "\t\tstate = torch.tensor(state, dtype=torch.float32).reshape(1, -1)\n",
    "\t\tprobs = policy(state)\n",
    "\t\taction = torch.multinomial(probs, 1).item()\n",
    "\t\tlog_prob = torch.log(probs[0, action])\n",
    "\n",
    "\t\t# Take step\n",
    "\t\tnext_state, reward, done, _, _ = env.step(action)\n",
    "\t\trewards.append(reward)\n",
    "\t\tlog_probs.append(log_prob)\n",
    "\t\tstate = next_state\n",
    "\t\t\n",
    "\t# Update policy\n",
    "\tif episode % 1000 == 0:\n",
    "\t\tprint(f\"Episode {episode}: {sum(rewards)}\")\n",
    "\tupdate_policy(rewards, log_probs, optimizer)\n",
    "\trewards = []\n",
    "\tlog_probs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 4 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "print(state)\n",
    "\n",
    "state = torch.tensor(state, dtype=torch.float32).reshape(1, -1)\n",
    "probs = policy(state)\n",
    "\n",
    "action = torch.multinomial(probs, 1).item()\n",
    "\n",
    "action\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
